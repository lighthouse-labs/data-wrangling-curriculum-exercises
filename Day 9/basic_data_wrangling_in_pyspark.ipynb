{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zb8Wv_7_nGni"
   },
   "source": [
    "# Beginners Guide to PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nZXXfyQ_oFfx"
   },
   "outputs": [],
   "source": [
    "# Let's start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('PySpark_Tutorial')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqDy5sW3qRfJ"
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8Y4xl9ktpqi"
   },
   "source": [
    "### Download DATA\n",
    "\n",
    "You can find the stock price data [here](https://drive.google.com/file/d/19z6AKWpKOQLpOiiLZ_QoprsPtIcOipNa/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZMQsAItvF9I"
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5elOd3_urwh"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nlaWsM6veh_"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "J7AR4YTjeR-6",
    "outputId": "a5dab4f8-606a-4ab4-fbf0-fb88c846aaa9"
   },
   "outputs": [],
   "source": [
    "# Before changing schema\n",
    "b_data = spark.read.csv(\n",
    "    'stocks_price_final.csv',\n",
    "    sep = ',',\n",
    "    header = True,\n",
    "    )\n",
    "\n",
    "b_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all data types are **strings**. Spark tries to infer the schema from data however some times the inferred datatype may not be correct or we may need to define our own column names and data types. We should assign the datatypes manually, similarly to what we do when creating table in SQL. Therefore it's important that we are familiar with the dataset before we load it to Spark. If it is huge it's always a good idea to explore the smaller sample locally in Excel or Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGT7jy3DDuLz"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data_schema = [\n",
    "               StructField('_c0', IntegerType(), True),\n",
    "               StructField('symbol', StringType(), True),\n",
    "               StructField('data', DateType(), True),\n",
    "               StructField('open', DoubleType(), True),\n",
    "               StructField('high', DoubleType(), True),\n",
    "               StructField('low', DoubleType(), True),\n",
    "               StructField('close', DoubleType(), True),\n",
    "               StructField('volume', IntegerType(), True),\n",
    "               StructField('adjusted', DoubleType(), True),\n",
    "               StructField('market.cap', StringType(), True),\n",
    "               StructField('sector', StringType(), True),\n",
    "               StructField('industry', StringType(), True),\n",
    "               StructField('exchange', StringType(), True),\n",
    "            ]\n",
    "\n",
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAThgJoIvcKI"
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv(\n",
    "    'stocks_price_final.csv',\n",
    "    sep = ',',\n",
    "    header = True,\n",
    "    schema = final_struc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "NCFGHCsUyamR",
    "outputId": "d74d286c-fe98-40b3-8576-64a71630c0c9"
   },
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can check the first 5 rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "bUgocSgd_gqy",
    "outputId": "0ef2e836-f2f2-41c2-e671-0ae8c87c9fd2"
   },
   "outputs": [],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "us8Ks-FEHTYe"
   },
   "outputs": [],
   "source": [
    "data = data.withColumnRenamed('market.cap', 'market_cap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0aFk3S-mN50"
   },
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**schema():** This method returns the schema of the data(dataframe). The below example w.r.t US StockPrice data is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "heGhvrD_mSZZ",
    "outputId": "5b629e83-b8e2-49a1-b7d2-dfc3f8c568d7"
   },
   "outputs": [],
   "source": [
    "data.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dtypes:** It returns a list of tuples with column names and it’s data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "11e0YeMXmhUs",
    "outputId": "87939907-5797-4994-9b29-897f885ce848"
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**head(n)**: It returns n rows as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "94m1hXVPprqU",
    "outputId": "630d15a5-8b20-4b22-ca96-64542fdd0c99"
   },
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show(n)**: this works in the exactly same way as **head()** in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "IYZ46WYNHc01",
    "outputId": "6d4fa243-b3b4-4b5d-9f45-8d30accf1d01"
   },
   "outputs": [],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**describe():** It computes the summary statistics of the columns with the numeric data type. It creates a dataframe so we need to call .show() to see the results in the output window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "ea-TUnvamQvg",
    "outputId": "106304e3-98fa-4b96-8abe-015c94f2d6c0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1a967824319f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# PySpark is lazily evaluated so it doesn't execute until we call function .show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# PySpark is lazily evaluated so it doesn't execute until we call function .show()\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**columns:** It works in the exactly same way as Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "WEfAsBkRtn9W",
    "outputId": "2cb67156-45c4-4a84-8cae-f46e064d1b77"
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count(): It returns the count of the number of rows in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "IMZZZMo_tppK",
    "outputId": "ac3c1bfa-71a9-4e98-94ff-31666476b086"
   },
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add distinct() before count, it will give us only distinct rows. This is the ideal way to check for potential duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JVkU70BotpxY",
    "outputId": "7e9ccad7-c103-450a-ddb3-ad6dda8bb0c0"
   },
   "outputs": [],
   "source": [
    "data.distinct().count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww3XqquewkvB"
   },
   "source": [
    "## Column Operations/Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will take a look at how we can manipulate columns using PySpark, specifically at:\n",
    "\n",
    "- adding columns\n",
    "- updating columns\n",
    "- deleting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "b7NNmtMYwka1",
    "outputId": "3ed5dac0-8ebe-4b95-88bb-114a1ce212be"
   },
   "outputs": [],
   "source": [
    "# adding column date\n",
    "data = data.withColumn('date', data.data)\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "gIpRmPSNxILh",
    "outputId": "665da690-8d48-4609-894c-d2220bc50605"
   },
   "outputs": [],
   "source": [
    "# renaming column date to data_changed\n",
    "data = data.withColumnRenamed('date', 'data_changed')\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "ZfMRq0AwxH3R",
    "outputId": "d70a40df-dc81-4568-ed80-e82511ec9423"
   },
   "outputs": [],
   "source": [
    "# dropping column data_changed\n",
    "data = data.drop('data_changed')\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Values\n",
    "\n",
    "Missing values are often part of the datasets we have to work with. As we already know there are bunch of ways we can use to handle the missing data, for example:\n",
    "\n",
    "- removing\n",
    "- imputing with Mean or Median\n",
    "- imputing with Most Frequent Value\n",
    "\n",
    "In Spark we can, of course, use all of this techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Rows with Missing Values\n",
    "data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Missing Values with Mean\n",
    "data.na.fill(data.select(f.mean(data['open'])).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Data\n",
    "\n",
    "The PySpark and PySpark SQL provide a wide range of methods and functions to query the data at ease. The idea is to reproduce SQL logic as close as possible. Here are the few most used methods:\n",
    "\n",
    "- Select\n",
    "- Filter\n",
    "- Between\n",
    "- When\n",
    "- Like\n",
    "- GroupBy\n",
    "- Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select\n",
    "It is used to select single or multiple columns using the names of the columns. Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "GRpy_6Fn29rJ",
    "outputId": "4870da3e-6c78-4dc3-aa12-1ce4cb1c543f"
   },
   "outputs": [],
   "source": [
    "data.select('sector').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "FTlxRGkw6jCu",
    "outputId": "43028000-9185-4b58-ed0d-30f9e5bd91dd"
   },
   "outputs": [],
   "source": [
    "data.select(['open', 'close', 'adjusted']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "Filter the data based on the given condition, you can also give multiple conditions using AND(**&**), OR(**|**), and NOT(**~**) operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "EsZfCrIZ6qkd",
    "outputId": "f1a63443-ed6c-432c-b220-81283a574ee5"
   },
   "outputs": [],
   "source": [
    "data.filter(data.adjusted.between(100.0, 500.0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "IYWm7iDy8dHC",
    "outputId": "df39d2c1-b00c-4b47-af0a-4afb44b1382d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "data.filter( (col('data') >= lit('2020-01-01')) & (col('data') <= lit('2020-01-31')) ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When\n",
    "It returns 0 or 1 depending on the given condition, the below example shows how to select the opening and closing price of stocks when the adjusted price is greater than equals to 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "i5LKNfTGAEl0",
    "outputId": "1c9066c5-e8ab-4447-e236-0582bcad6eb1"
   },
   "outputs": [],
   "source": [
    "data.select('open', 'close', f.when(data.adjusted >= 200.0, 1).otherwise(0)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like\n",
    "It is similar to the like operator in SQL, The below example show to extract the sector names which stars with either M or C using `rlike`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "exczz-BFBd9N",
    "outputId": "43641b12-f454-4d4c-b64f-f945117faf01"
   },
   "outputs": [],
   "source": [
    "data.select('sector', \n",
    "            data.sector.rlike('^[B,C]').alias('Sector Starting with B or C')\n",
    "            ).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy\n",
    "It groups the data by the given column name and it can perform different operations such as sum, mean, min, max, e.t.c. The below example explains how to get the average opening, closing, and adjusted stock price concerning industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "3MBAUrelDvBO",
    "outputId": "1e0e48c0-fdb1-4f57-de76-8db6350a17eb"
   },
   "outputs": [],
   "source": [
    "data.select(['industry', 'open', 'close', 'adjusted']).groupBy('industry').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkSdYUdRGkkk"
   },
   "source": [
    "### Aggregation\n",
    "This is similiar to GroupBy statement in SQL where we can call different aggregation functions like count, sum, min, max on different variables. The below example shows how to display the minimum, maximum, and average; opening, closing, and adjusted stock prices from January 2019 to January 2020 concerning the sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all pyspark sql functions as fs\n",
    "from pyspark.sql import functions as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter( (col('data') >= lit('2019-01-02')) & (col('data') <= lit('2020-01-31')) )\\\n",
    "    .groupBy(\"sector\") \\\n",
    "    .agg(fs.min(\"data\").alias(\"From\"), \n",
    "         fs.max(\"data\").alias(\"To\"), \n",
    "         \n",
    "         fs.min(\"open\").alias(\"Minimum Opening\"),\n",
    "         fs.max(\"open\").alias(\"Maximum Opening\"), \n",
    "         fs.avg(\"open\").alias(\"Average Opening\"), \n",
    "\n",
    "         fs.min(\"close\").alias(\"Minimum Closing\"), \n",
    "         fs.max(\"close\").alias(\"Maximum Closing\"), \n",
    "         fs.avg(\"close\").alias(\"Average Closing\"), \n",
    "\n",
    "         fs.min(\"adjusted\").alias(\"Minimum Adjusted Closing\"), \n",
    "         fs.max(\"adjusted\").alias(\"Maximum Adjusted Closing\"), \n",
    "         fs.avg(\"adjusted\").alias(\"Average Adjusted Closing\"), \n",
    "\n",
    "      ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "We will combine PySpark with Matplotlib and Pandas to show simple visualizations.  `toPandas()` method is ised to convert the data into pandas dataframe. Using the dataframe we utilize the `plot()` method to visualize data. When we are working with huge data, it's important to do some aggregation before transforming data into Pandas dataframe. \n",
    "\n",
    "The below code shows how to display a bar graph for the average opening, closing, and adjusted stock price concerning the sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df =  data.select(['sector', 'open', 'close', 'adjusted']).groupBy('sector').mean().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(12))\n",
    "\n",
    "ind.pop(6)\n",
    "\n",
    "sec_df.iloc[ind ,:].plot(kind = 'bar', x='sector', y = sec_df.columns.tolist()[1:], \n",
    "                         figsize=(12, 6), ylabel = 'Stock Price', xlabel = 'Sector')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, let’s visualize the average opening, closing, and adjusted price concerning industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries_x = data.select(['industry', 'open', 'close', 'adjusted']).groupBy('industry').mean().toPandas()\n",
    "q  = industries_x[(industries_x.industry != 'Major Chemicals') & (industries_x.industry != 'Building Products')]\n",
    "q.plot(kind = 'barh', x='industry', y = q.columns.tolist()[1:], figsize=(10, 50), xlabel='Stock Price', ylabel = 'Industry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write/Save Data to File\n",
    "\n",
    "The `write.save()` method is used to save the data in different formats such as CSV, JSON, Parquet, e.t.c. Let’s see how to save the data in different file formats. We can able to save entire data and selected data using the `select()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Writing entire data to different file formats\n",
    "\n",
    "# CSV\n",
    "data.write.csv('dataset.csv')\n",
    "\n",
    "# JSON\n",
    "data.write.save('dataset.json', format='json')\n",
    "\n",
    "# Parquet\n",
    "data.write.save('dataset.parquet', format='parquet')\n",
    "\n",
    "## Writing selected data to different file formats\n",
    "\n",
    "# CSV\n",
    "data.select(['data', 'open', 'close', 'adjusted'])\\\n",
    "            .write.csv('dataset.csv')\n",
    "\n",
    "# JSON\n",
    "data.select(['data', 'open', 'close', 'adjusted'])\\\n",
    "    .write.save('dataset.json', format='json')\n",
    "\n",
    "# Parquet\n",
    "data.select(['data', 'open', 'close', 'adjusted'])\\\n",
    "    .write.save('dataset.parquet', format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPRiFrIk3poFvvDKdeo/sex",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Beginners Guide to PySpark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
